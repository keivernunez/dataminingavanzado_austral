{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keivernunez/dataminingavanzado_austral/blob/main/Clase01_Note_2_de_5_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Uu8IEMfRPz"
      },
      "source": [
        "# M치quinas de Vectores de Soporte (SVM): Gu칤a Te칩rica y Pr치ctica\n",
        "\n",
        "**Introducci칩n:**\n",
        "\n",
        "Este cuaderno proporciona una introducci칩n exhaustiva a las M치quinas de Vectores de Soporte (SVM), un algoritmo de aprendizaje supervisado de gran potencia, utilizado principalmente para problemas de clasificaci칩n como tambi칠n de regresi칩n. Las SVM son particularmente efectivas en espacios de alta dimensionalidad y cuando el n칰mero de dimensiones excede el n칰mero de muestras.\n",
        "\n",
        "El objetivo fundamental de SVM es determinar un **hiperplano** 칩ptimo que separe las distintas clases dentro del espacio de caracter칤sticas. En este cuaderno, se explorar치n los conceptos fundamentales, se mostrar치 un ejemplo pr치ctico de clasificaci칩n con el conjunto de datos del Titanic, se visualizar치n las fronteras de decisi칩n y se incluir치 un ejemplo de clustering para determinar un n칰mero 칩ptimo de agrupaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUmt0rCPfRP3"
      },
      "source": [
        "## Conceptos Fundamentales de SVM\n",
        "\n",
        "Una SVM busca un hiperplano que separe las clases con el margen m치ximo posible. En un problema de clasificaci칩n binaria, este hiperplano es una superficie (una l칤nea en 2D, un plano en 3D) que divide el espacio de caracter칤sticas.\n",
        "\n",
        "**1. Hiperplano:** Es el plano de decisi칩n que segrega las clases. En un problema con 2 caracter칤sticas (2D), se trata de una l칤nea. Con 3 caracter칤sticas, es un plano; con m치s de tres, se generaliza al concepto de hiperplano.\n",
        "\n",
        "**2. Vectores de Soporte (Support Vectors):** Corresponden a las instancias de datos de cada clase que se encuentran **m치s pr칩ximas** al hiperplano de decisi칩n. Estos puntos son cruciales, ya que son los m치s complejos de clasificar y definen de manera un칤voca la posici칩n y orientaci칩n del hiperplano.\n",
        "\n",
        "**3. Margen (Margin):** Es la distancia perpendicular entre el hiperplano y los vectores de soporte m치s cercanos. El objetivo de SVM no es simplemente separar las clases, sino hacerlo con el **margen m치s amplio posible (Maximum-Margin Hyperplane)**. Un margen amplio se correlaciona con una mejor capacidad de generalizaci칩n del modelo, haci칠ndolo m치s robusto frente a datos no vistos.\n",
        "\n",
        "**4. M치rgenes Duros vs. Suaves (Hard vs. Soft Margins):**\n",
        "    - **Margen Duro:** Impone una restricci칩n estricta: no se permite ning칰n error de clasificaci칩n. Este enfoque solo es viable si los datos son perfectamente separables de forma lineal, una condici칩n infrecuente en problemas reales.\n",
        "    - **Margen Suave:** Introduce flexibilidad al permitir que algunas instancias sean clasificadas incorrectamente o queden dentro del margen. Esto es esencial para manejar datos con ruido o solapamiento entre clases. El hiperpar치metro de regularizaci칩n `C` controla este compromiso:\n",
        "        - Un valor de `C` **elevado** impone una alta penalizaci칩n a los errores, resultando en un margen m치s estricto y un modelo con menor sesgo pero mayor varianza (riesgo de sobreajuste).\n",
        "        - Un valor de `C` **bajo** permite una mayor cantidad de errores, generando un margen m치s suave y un modelo con mayor sesgo pero menor varianza (riesgo de subajuste)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdNTfexfRP3"
      },
      "source": [
        "### El Truco del Kernel (Kernel Trick) 游먹랻\n",
        "\n",
        "Cuando los datos no son separables mediante un hiperplano lineal en su espacio original, SVM emplea una de sus caracter칤sticas m치s potentes: el **truco del kernel**.\n",
        "\n",
        "Un *kernel* es una funci칩n matem치tica que calcula el producto escalar entre dos vectores en un espacio de caracter칤sticas de mayor dimensionalidad, sin necesidad de realizar la transformaci칩n expl칤cita de los datos a dicho espacio. Esto permite encontrar un hiperplano separador en ese espacio de mayor dimensi칩n de manera computacionalmente eficiente.\n",
        "\n",
        "**Kernels m치s comunes:**\n",
        "- **Lineal (`linear`):** No aplica ninguna transformaci칩n. Es eficiente y recomendable como punto de partida si se sospecha que la relaci칩n es lineal.\n",
        "- **Polinomial (`poly`):** Modela interacciones polin칩micas. Sus hiperpar치metros incluyen el grado del polinomio (`degree`).\n",
        "- **Base Radial (RBF - `rbf`):** Es el kernel m치s utilizado y potente por su capacidad para manejar relaciones no lineales complejas. Su hiperpar치metro `gamma` define la influencia de un 칰nico ejemplo de entrenamiento. Un `gamma` alto puede llevar a sobreajuste, mientras que un `gamma` bajo puede resultar en subajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjSPXF7FfRP4"
      },
      "outputs": [],
      "source": [
        "# Importaciones est치ndar y configuraci칩n del entorno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, RocCurveDisplay\n",
        ")\n",
        "from sklearn.datasets import make_moons, load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import warnings\n",
        "\n",
        "# Configuraciones generales\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNfbD5lBfRP5"
      },
      "source": [
        "## Ejemplo Pr치ctico: Clasificaci칩n SVM con el Dataset Titanic\n",
        "\n",
        "Se aplicar치 un clasificador SVM para predecir la supervivencia de los pasajeros del Titanic. Para ello, se seguir치 un flujo de trabajo estructurado que incluye:\n",
        "1. Carga y preprocesamiento de datos.\n",
        "2. Construcci칩n de un `Pipeline` para encapsular la transformaci칩n de datos y el modelo.\n",
        "3. B칰squeda de hiperpar치metros 칩ptimos (`C` y `gamma`) mediante `GridSearchCV`.\n",
        "4. Evaluaci칩n del rendimiento del modelo final con diversas m칠tricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCZ4MajVfRP5"
      },
      "outputs": [],
      "source": [
        "# Carga y preparaci칩n del dataset Titanic\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Se seleccionan las caracter칤sticas relevantes y se eliminan filas con valores nulos en 'embarked'\n",
        "titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].copy()\n",
        "titanic = titanic.dropna(subset=['embarked'])\n",
        "\n",
        "# Definici칩n de caracter칤sticas (X) y variable objetivo (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Identificaci칩n de tipos de columnas para el preprocesamiento\n",
        "num_cols = ['age', 'sibsp', 'parch', 'fare']\n",
        "cat_cols = ['pclass', 'sex', 'embarked']\n",
        "\n",
        "# Divisi칩n en conjuntos de entrenamiento y prueba (train/test split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLcpP2klfRP6"
      },
      "outputs": [],
      "source": [
        "# Creaci칩n del pipeline de preprocesamiento para variables num칠ricas y categ칩ricas\n",
        "\n",
        "# Pipeline para datos num칠ricos: imputaci칩n de mediana y escalado est치ndar\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para datos categ칩ricos: imputaci칩n de moda y codificaci칩n one-hot\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combinaci칩n de ambos pipelines en un 칰nico ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_cols),\n",
        "    ('cat', cat_pipeline, cat_cols)\n",
        "])\n",
        "\n",
        "# Creaci칩n del pipeline final que integra el preprocesador y el clasificador SVM\n",
        "pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIqzkn98fRP6"
      },
      "outputs": [],
      "source": [
        "# B칰squeda en grilla (Grid Search) para los hiperpar치metros C y gamma\n",
        "param_grid = {\n",
        "    'clf__C': [0.1, 1, 10, 100],\n",
        "    'clf__gamma': ['scale', 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Se utiliza validaci칩n cruzada (cv=5) y se optimiza para el 치rea bajo la curva ROC (roc_auc)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f'Mejores par치metros encontrados: {grid.best_params_}')\n",
        "print(f'Mejor ROC AUC en validaci칩n cruzada: {grid.best_score_:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW-1l8eQfRP6"
      },
      "outputs": [],
      "source": [
        "# Evaluaci칩n del mejor modelo en el conjunto de prueba\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# M칠tricas de rendimiento\n",
        "print(f'Exactitud (Accuracy) en prueba: {(y_pred == y_test).mean():.4f}')\n",
        "print('\\nReporte de Clasificaci칩n:\\n', classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusi칩n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusi칩n')\n",
        "plt.xlabel('Predicci칩n')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwKcifZNfRP7"
      },
      "outputs": [],
      "source": [
        "# Visualizaci칩n de la Curva ROC y c치lculo del AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f'츼rea Bajo la Curva (AUC) en prueba: {roc_auc:.4f}')\n",
        "\n",
        "# Gr치fico de la curva ROC\n",
        "RocCurveDisplay.from_estimator(best_model, X_test, y_test)\n",
        "plt.title('Curva ROC (Receiver Operating Characteristic)')\n",
        "plt.plot([0, 1], [0, 1], 'r--', label='Clasificador Aleatorio')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IMaDzTwfRP7"
      },
      "source": [
        "## Visualizaci칩n de Fronteras de Decisi칩n\n",
        "\n",
        "Para una comprensi칩n m치s intuitiva de c칩mo funcionan los diferentes kernels y el hiperpar치metro `C`, se utilizar치 un conjunto de datos sint칠tico en 2D. Esto permite visualizar directamente el hiperplano y los m치rgenes que el modelo aprende."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07CJ91swfRP7"
      },
      "outputs": [],
      "source": [
        "# Creaci칩n de un dataset sint칠tico no linealmente separable\n",
        "X_synth, y_synth = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "X_synth_scaled = StandardScaler().fit_transform(X_synth)\n",
        "\n",
        "# Funci칩n auxiliar para visualizar la frontera de decisi칩n\n",
        "def plot_decision_boundary(clf, X, y, title):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Caracter칤stica 1 (escalada)')\n",
        "    plt.ylabel('Caracter칤stica 2 (escalada)')\n",
        "    plt.show()\n",
        "\n",
        "# Entrenamiento y visualizaci칩n de diferentes modelos SVM\n",
        "svm_linear = SVC(kernel='linear', C=1).fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_linear, X_synth_scaled, y_synth, 'SVM con Kernel Lineal (C=1)')\n",
        "\n",
        "svm_rbf_low_c = SVC(kernel='rbf', C=0.1, gamma='auto').fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_rbf_low_c, X_synth_scaled, y_synth, 'SVM con Kernel RBF (C=0.1, Margen Suave)')\n",
        "\n",
        "svm_rbf_high_c = SVC(kernel='rbf', C=100, gamma='auto').fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_rbf_high_c, X_synth_scaled, y_synth, 'SVM con Kernel RBF (C=100, Margen Duro)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUYXNSU3fRP7"
      },
      "source": [
        "## Ejemplo de Clustering: M칠todo del Codo y Silueta con el Dataset Iris\n",
        "\n",
        "Aunque las SVM son principalmente para clasificaci칩n, el an치lisis de clustering es una t칠cnica fundamental en el aprendizaje no supervisado. Se utilizar치n los m칠todos del **Codo** y de la **Silueta** para determinar el n칰mero 칩ptimo de clusters en el cl치sico dataset Iris."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK-v07gsfRP7"
      },
      "outputs": [],
      "source": [
        "# Carga del dataset Iris\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# M칠todo del Codo para encontrar el n칰mero 칩ptimo de clusters (k)\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_iris)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('N칰mero de Clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('M칠todo del Codo para Selecci칩n de k')\n",
        "plt.xticks(k_range)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCpp2X6EfRP8"
      },
      "outputs": [],
      "source": [
        "# An치lisis de Silueta para k=3 (valor sugerido por el m칠todo del codo)\n",
        "best_k = 3\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X_iris)\n",
        "\n",
        "silhouette_avg = silhouette_score(X_iris, labels)\n",
        "print(f'Para k = {best_k}, el puntaje de silueta promedio es: {silhouette_avg:.4f}')\n",
        "\n",
        "# Gr치fico de Silueta\n",
        "silhouette_vals = silhouette_samples(X_iris, labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "y_lower = 10\n",
        "for i in range(best_k):\n",
        "    ith_cluster_sil_vals = silhouette_vals[labels == i]\n",
        "    ith_cluster_sil_vals.sort()\n",
        "    size_cluster_i = ith_cluster_sil_vals.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_vals)\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.axvline(x=silhouette_avg, color='red', linestyle='--')\n",
        "plt.xlabel('Valores del coeficiente de Silueta')\n",
        "plt.ylabel('Etiqueta del Cluster')\n",
        "plt.title(f'Gr치fico de Silueta para k = {best_k}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRhbVc7ifRP8"
      },
      "source": [
        "## Resumen y Sugerencias\n",
        "\n",
        "- Este cuaderno ha cubierto los conceptos fundamentales de SVM, incluyendo un ejemplo pr치ctico de clasificaci칩n con un pipeline limpio, b칰squeda de hiperpar치metros, evaluaci칩n con ROC/AUC y visualizaci칩n de fronteras de decisi칩n.\n",
        "- Se ha demostrado el poder de los pipelines de Scikit-learn para crear flujos de trabajo de machine learning robustos y reproducibles.\n",
        "- Para clustering, se han mostrado los m칠todos del Codo y de la Silueta como herramientas para la selecci칩n del n칰mero 칩ptimo de clusters, siendo el an치lisis de silueta a menudo m치s informativo.\n",
        "\n",
        "**Sugerencias para extender el an치lisis:**\n",
        "- Implementar curvas ROC validadas cruzadamente (con `StratifiedKFold`).\n",
        "- Probar diferentes kernels (ej. `poly`) y visualizar el efecto en los m치rgenes.\n",
        "- Para problemas de clasificaci칩n multiclase, utilizar estrategias como One-vs-Rest y promediado micro/macro de las m칠tricas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}