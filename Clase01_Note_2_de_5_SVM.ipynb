{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keivernunez/dataminingavanzado_austral/blob/main/Clase01_Note_2_de_5_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Uu8IEMfRPz"
      },
      "source": [
        "# Máquinas de Vectores de Soporte (SVM): Guía Teórica y Práctica\n",
        "\n",
        "**Introducción:**\n",
        "\n",
        "Este cuaderno proporciona una introducción exhaustiva a las Máquinas de Vectores de Soporte (SVM), un algoritmo de aprendizaje supervisado de gran potencia, utilizado principalmente para problemas de clasificación como también de regresión. Las SVM son particularmente efectivas en espacios de alta dimensionalidad y cuando el número de dimensiones excede el número de muestras.\n",
        "\n",
        "El objetivo fundamental de SVM es determinar un **hiperplano** óptimo que separe las distintas clases dentro del espacio de características. En este cuaderno, se explorarán los conceptos fundamentales, se mostrará un ejemplo práctico de clasificación con el conjunto de datos del Titanic, se visualizarán las fronteras de decisión y se incluirá un ejemplo de clustering para determinar un número óptimo de agrupaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUmt0rCPfRP3"
      },
      "source": [
        "## Conceptos Fundamentales de SVM\n",
        "\n",
        "Una SVM busca un hiperplano que separe las clases con el margen máximo posible. En un problema de clasificación binaria, este hiperplano es una superficie (una línea en 2D, un plano en 3D) que divide el espacio de características.\n",
        "\n",
        "**1. Hiperplano:** Es el plano de decisión que segrega las clases. En un problema con 2 características (2D), se trata de una línea. Con 3 características, es un plano; con más de tres, se generaliza al concepto de hiperplano.\n",
        "\n",
        "**2. Vectores de Soporte (Support Vectors):** Corresponden a las instancias de datos de cada clase que se encuentran **más próximas** al hiperplano de decisión. Estos puntos son cruciales, ya que son los más complejos de clasificar y definen de manera unívoca la posición y orientación del hiperplano.\n",
        "\n",
        "**3. Margen (Margin):** Es la distancia perpendicular entre el hiperplano y los vectores de soporte más cercanos. El objetivo de SVM no es simplemente separar las clases, sino hacerlo con el **margen más amplio posible (Maximum-Margin Hyperplane)**. Un margen amplio se correlaciona con una mejor capacidad de generalización del modelo, haciéndolo más robusto frente a datos no vistos.\n",
        "\n",
        "**4. Márgenes Duros vs. Suaves (Hard vs. Soft Margins):**\n",
        "    - **Margen Duro:** Impone una restricción estricta: no se permite ningún error de clasificación. Este enfoque solo es viable si los datos son perfectamente separables de forma lineal, una condición infrecuente en problemas reales.\n",
        "    - **Margen Suave:** Introduce flexibilidad al permitir que algunas instancias sean clasificadas incorrectamente o queden dentro del margen. Esto es esencial para manejar datos con ruido o solapamiento entre clases. El hiperparámetro de regularización `C` controla este compromiso:\n",
        "        - Un valor de `C` **elevado** impone una alta penalización a los errores, resultando en un margen más estricto y un modelo con menor sesgo pero mayor varianza (riesgo de sobreajuste).\n",
        "        - Un valor de `C` **bajo** permite una mayor cantidad de errores, generando un margen más suave y un modelo con mayor sesgo pero menor varianza (riesgo de subajuste)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdNTfexfRP3"
      },
      "source": [
        "### El Truco del Kernel (Kernel Trick) 🧠✨\n",
        "\n",
        "Cuando los datos no son separables mediante un hiperplano lineal en su espacio original, SVM emplea una de sus características más potentes: el **truco del kernel**.\n",
        "\n",
        "Un *kernel* es una función matemática que calcula el producto escalar entre dos vectores en un espacio de características de mayor dimensionalidad, sin necesidad de realizar la transformación explícita de los datos a dicho espacio. Esto permite encontrar un hiperplano separador en ese espacio de mayor dimensión de manera computacionalmente eficiente.\n",
        "\n",
        "**Kernels más comunes:**\n",
        "- **Lineal (`linear`):** No aplica ninguna transformación. Es eficiente y recomendable como punto de partida si se sospecha que la relación es lineal.\n",
        "- **Polinomial (`poly`):** Modela interacciones polinómicas. Sus hiperparámetros incluyen el grado del polinomio (`degree`).\n",
        "- **Base Radial (RBF - `rbf`):** Es el kernel más utilizado y potente por su capacidad para manejar relaciones no lineales complejas. Su hiperparámetro `gamma` define la influencia de un único ejemplo de entrenamiento. Un `gamma` alto puede llevar a sobreajuste, mientras que un `gamma` bajo puede resultar en subajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjSPXF7FfRP4"
      },
      "outputs": [],
      "source": [
        "# Importaciones estándar y configuración del entorno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, RocCurveDisplay\n",
        ")\n",
        "from sklearn.datasets import make_moons, load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import warnings\n",
        "\n",
        "# Configuraciones generales\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNfbD5lBfRP5"
      },
      "source": [
        "## Ejemplo Práctico: Clasificación SVM con el Dataset Titanic\n",
        "\n",
        "Se aplicará un clasificador SVM para predecir la supervivencia de los pasajeros del Titanic. Para ello, se seguirá un flujo de trabajo estructurado que incluye:\n",
        "1. Carga y preprocesamiento de datos.\n",
        "2. Construcción de un `Pipeline` para encapsular la transformación de datos y el modelo.\n",
        "3. Búsqueda de hiperparámetros óptimos (`C` y `gamma`) mediante `GridSearchCV`.\n",
        "4. Evaluación del rendimiento del modelo final con diversas métricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCZ4MajVfRP5"
      },
      "outputs": [],
      "source": [
        "# Carga y preparación del dataset Titanic\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Se seleccionan las características relevantes y se eliminan filas con valores nulos en 'embarked'\n",
        "titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].copy()\n",
        "titanic = titanic.dropna(subset=['embarked'])\n",
        "\n",
        "# Definición de características (X) y variable objetivo (y)\n",
        "X = titanic.drop(columns=['survived'])\n",
        "y = titanic['survived']\n",
        "\n",
        "# Identificación de tipos de columnas para el preprocesamiento\n",
        "num_cols = ['age', 'sibsp', 'parch', 'fare']\n",
        "cat_cols = ['pclass', 'sex', 'embarked']\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba (train/test split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLcpP2klfRP6"
      },
      "outputs": [],
      "source": [
        "# Creación del pipeline de preprocesamiento para variables numéricas y categóricas\n",
        "\n",
        "# Pipeline para datos numéricos: imputación de mediana y escalado estándar\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para datos categóricos: imputación de moda y codificación one-hot\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combinación de ambos pipelines en un único ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_cols),\n",
        "    ('cat', cat_pipeline, cat_cols)\n",
        "])\n",
        "\n",
        "# Creación del pipeline final que integra el preprocesador y el clasificador SVM\n",
        "pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIqzkn98fRP6"
      },
      "outputs": [],
      "source": [
        "# Búsqueda en grilla (Grid Search) para los hiperparámetros C y gamma\n",
        "param_grid = {\n",
        "    'clf__C': [0.1, 1, 10, 100],\n",
        "    'clf__gamma': ['scale', 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Se utiliza validación cruzada (cv=5) y se optimiza para el área bajo la curva ROC (roc_auc)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f'Mejores parámetros encontrados: {grid.best_params_}')\n",
        "print(f'Mejor ROC AUC en validación cruzada: {grid.best_score_:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW-1l8eQfRP6"
      },
      "outputs": [],
      "source": [
        "# Evaluación del mejor modelo en el conjunto de prueba\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Métricas de rendimiento\n",
        "print(f'Exactitud (Accuracy) en prueba: {(y_pred == y_test).mean():.4f}')\n",
        "print('\\nReporte de Clasificación:\\n', classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Valor Real')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwKcifZNfRP7"
      },
      "outputs": [],
      "source": [
        "# Visualización de la Curva ROC y cálculo del AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f'Área Bajo la Curva (AUC) en prueba: {roc_auc:.4f}')\n",
        "\n",
        "# Gráfico de la curva ROC\n",
        "RocCurveDisplay.from_estimator(best_model, X_test, y_test)\n",
        "plt.title('Curva ROC (Receiver Operating Characteristic)')\n",
        "plt.plot([0, 1], [0, 1], 'r--', label='Clasificador Aleatorio')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IMaDzTwfRP7"
      },
      "source": [
        "## Visualización de Fronteras de Decisión\n",
        "\n",
        "Para una comprensión más intuitiva de cómo funcionan los diferentes kernels y el hiperparámetro `C`, se utilizará un conjunto de datos sintético en 2D. Esto permite visualizar directamente el hiperplano y los márgenes que el modelo aprende."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07CJ91swfRP7"
      },
      "outputs": [],
      "source": [
        "# Creación de un dataset sintético no linealmente separable\n",
        "X_synth, y_synth = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "X_synth_scaled = StandardScaler().fit_transform(X_synth)\n",
        "\n",
        "# Función auxiliar para visualizar la frontera de decisión\n",
        "def plot_decision_boundary(clf, X, y, title):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Característica 1 (escalada)')\n",
        "    plt.ylabel('Característica 2 (escalada)')\n",
        "    plt.show()\n",
        "\n",
        "# Entrenamiento y visualización de diferentes modelos SVM\n",
        "svm_linear = SVC(kernel='linear', C=1).fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_linear, X_synth_scaled, y_synth, 'SVM con Kernel Lineal (C=1)')\n",
        "\n",
        "svm_rbf_low_c = SVC(kernel='rbf', C=0.1, gamma='auto').fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_rbf_low_c, X_synth_scaled, y_synth, 'SVM con Kernel RBF (C=0.1, Margen Suave)')\n",
        "\n",
        "svm_rbf_high_c = SVC(kernel='rbf', C=100, gamma='auto').fit(X_synth_scaled, y_synth)\n",
        "plot_decision_boundary(svm_rbf_high_c, X_synth_scaled, y_synth, 'SVM con Kernel RBF (C=100, Margen Duro)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUYXNSU3fRP7"
      },
      "source": [
        "## Ejemplo de Clustering: Método del Codo y Silueta con el Dataset Iris\n",
        "\n",
        "Aunque las SVM son principalmente para clasificación, el análisis de clustering es una técnica fundamental en el aprendizaje no supervisado. Se utilizarán los métodos del **Codo** y de la **Silueta** para determinar el número óptimo de clusters en el clásico dataset Iris."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK-v07gsfRP7"
      },
      "outputs": [],
      "source": [
        "# Carga del dataset Iris\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "\n",
        "# Método del Codo para encontrar el número óptimo de clusters (k)\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_iris)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Número de Clusters (k)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Método del Codo para Selección de k')\n",
        "plt.xticks(k_range)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCpp2X6EfRP8"
      },
      "outputs": [],
      "source": [
        "# Análisis de Silueta para k=3 (valor sugerido por el método del codo)\n",
        "best_k = 3\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X_iris)\n",
        "\n",
        "silhouette_avg = silhouette_score(X_iris, labels)\n",
        "print(f'Para k = {best_k}, el puntaje de silueta promedio es: {silhouette_avg:.4f}')\n",
        "\n",
        "# Gráfico de Silueta\n",
        "silhouette_vals = silhouette_samples(X_iris, labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "y_lower = 10\n",
        "for i in range(best_k):\n",
        "    ith_cluster_sil_vals = silhouette_vals[labels == i]\n",
        "    ith_cluster_sil_vals.sort()\n",
        "    size_cluster_i = ith_cluster_sil_vals.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_vals)\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.axvline(x=silhouette_avg, color='red', linestyle='--')\n",
        "plt.xlabel('Valores del coeficiente de Silueta')\n",
        "plt.ylabel('Etiqueta del Cluster')\n",
        "plt.title(f'Gráfico de Silueta para k = {best_k}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRhbVc7ifRP8"
      },
      "source": [
        "## Resumen y Sugerencias\n",
        "\n",
        "- Este cuaderno ha cubierto los conceptos fundamentales de SVM, incluyendo un ejemplo práctico de clasificación con un pipeline limpio, búsqueda de hiperparámetros, evaluación con ROC/AUC y visualización de fronteras de decisión.\n",
        "- Se ha demostrado el poder de los pipelines de Scikit-learn para crear flujos de trabajo de machine learning robustos y reproducibles.\n",
        "- Para clustering, se han mostrado los métodos del Codo y de la Silueta como herramientas para la selección del número óptimo de clusters, siendo el análisis de silueta a menudo más informativo.\n",
        "\n",
        "**Sugerencias para extender el análisis:**\n",
        "- Implementar curvas ROC validadas cruzadamente (con `StratifiedKFold`).\n",
        "- Probar diferentes kernels (ej. `poly`) y visualizar el efecto en los márgenes.\n",
        "- Para problemas de clasificación multiclase, utilizar estrategias como One-vs-Rest y promediado micro/macro de las métricas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}